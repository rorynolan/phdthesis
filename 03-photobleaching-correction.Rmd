# Photobleaching Correction {#photobleaching-correction}


## Introduction to photobleaching 

In the ideal case, an _incident_ photon of appropriate wavelength is absorbed by a fluorophore, promoting the fluorophore to an excited state; subsequently, the fluorophore relaxes down to its ground state by emitting a photon. In reality, it is possible that the incident photon can _break_ the fluorophore with the result that it will no longer emit light. This breaking is referred to as _photobleaching_ (or _bleaching_ for short). Bleaching causes a diminution in the number of effective fluorophores, which is a direct cause of a loss of fluorescent signal. 

Many quantitative methods in fields such as fluorescence fluctuation spectroscopy (FFS) and fluorescence correlation spectroscopy (FCS) implicitly assume that there is no bleaching in the data. Hence, data (image series) with significant levels of photobleaching must be corrected prior to the application of equations and algorithms in these fields. A main focus of this thesis is on how to correct fluorescent image series for the effects of bleaching, given that bleaching _does_ occur. There is no attempt to understand _why_ and/or _how_ photobleaching occurs.

```{remark}
All of the current literature mentions bleaching correction as being purely for correcting the problem of non-stationary mean (NSM), neglecting the problem of non-stationary variance (NSV). Figure \@ref(fig:visualize-variance-decrease) shows that NSM and NSV go hand-in-hand. Correction for NSM is referred to as trend removal or _detrending_. Hence, the terms _detrending_ and _bleaching correction_ have come to be used interchangeably. I will follow this convention and use the term _detrending_ from now on to mean correction for NSM and possibly also NSV. Starting at section \@ref(exponential-fitting-detrending), the focus is on correcting for NSM. Discussion of correction for NSV starts in section \@ref(correcting-for-non-stationary-variance).
```


## The effects of bleaching in FCS and FFS

We simulate two image series with 500 frames, 64x64 pixels per frame (pixel size 1 $\mu$m), each with 100,000 diffusing ($D = 100~\mu\text{m}^2/\text{s}$) fluorescent particles which are uniformly distributed at the beginning. In the first image series (`img1`), these have brightness $\epsilon = 4$ and in the second (`img2`) they have brightness $\epsilon = 7$. See figure \@ref(fig:bleaching-sim). We bleach these by 15% and 20% to create `img1_bleached` and `img2_bleached` respectively.

(ref:bleaching-sim-caption) Frames 1, 100, 200, 300, 400 and 500 from simulated image series. $\epsilon = 4$ (top) and $\epsilon = 7$ (bottom) with 15% (top) and 20% (bottom) bleaching. 

```{r bleaching-sim, fig.cap='(ref:bleaching-sim-caption)', echo=FALSE, fig.asp=0.5}
img1 <- read_tif(here("img", "sim1.tif"), msg = FALSE)
img1_bleached <- read_tif(here("img", "sim1_bleached.tif"), msg = FALSE)
img2 <- read_tif(here("img", "sim2.tif"), msg = FALSE)
img2_bleached <- read_tif(here("img", "sim2_bleached.tif"), msg = FALSE)
mx <- max(c(img1, img2, img1_bleached, img2_bleached))
sidebyside3space <- function(arr3d, mx) {
  lst <- map(seq_len(dim(arr3d)[3]), ~ arr3d[, , .])
  reduce(lst, ~ cbind(.x, mx, mx, mx, .y))
}
frames <- c(1, 100, 200, 300, 400, 500)
to_display <- rbind(sidebyside3space(img1_bleached[, , 1, frames], mx),
                    mx, mx, mx, mx, mx,
                    sidebyside3space(img2_bleached[, , 1, frames], mx))
ijtiff::display(to_display)
```


It may not be obvious that these image series are subject to bleaching from figure \@ref(fig:bleaching-sim), but we can see it more clearly in figure \@ref(fig:bleaching-sim-mean-intensity).

(ref:bleaching-sim-mean-intensity-caption) Mean intensity profiles of the simulated image series with and without bleaching. Simulation 1 was bleached by 15% and simulation 2 by 20%. The bleaching is such that the mean intensity decreases according to a single exponential decay (even though it looks quite linear).

```{r bleaching-sim-mean-intensity, fig.cap='(ref:bleaching-sim-mean-intensity-caption)', echo=FALSE}
df_bleach_sim <- tibble(simulation = c("img1", "img1_bleached",
                                      "img2", "img2_bleached"),
                       img = list(img1, img1_bleached, img2, img2_bleached),
                       frame = map(img, ~ seq_len(dim(.)[4])),
                       mean_intensity = map(img, apply, 4, mean)) %>% 
  select(-img) %>% 
  unnest(cols = c(frame, mean_intensity))
ggplot(df_bleach_sim, aes(x = frame, y = mean_intensity, colour = simulation)) +
  geom_line() + ylab("mean intensity") + 
  ylim(50, max(df_bleach_sim$mean_intensity))
```


### FCS

The unrelated images `img1` and `img2` have a tiny median cross-correlated brightness of $B_\text{cc} = `r median(cc_brightness(abind(img1, img2, along = 3))) %>% round(4)`$, signifying no significant correlation, as one would expect. However, the bleached images `img1_bleached` and `img2_bleached` have a significant $B_\text{cc} = `r median(cc_brightness(abind(img1_bleached, img2_bleached, along = 3))) %>% round(4)`$. This shows that bleaching is introducing correlation between otherwise unrelated images. Since correlation is used as a proxy for hetero-interaction, bleaching can make it appear as though there is interaction when in fact there is not.

#### Why does bleaching introduce correlation

Let's take another look at the formula for correlation:

```{definition, name='FCS'}
The _correlation_ between two random variables $X$ and $Y$ with expected values $\mu_X$ and $\mu_Y$ and standard deviations $\sigma_X$ and $\sigma_Y$ is
\begin{equation}
\text{corr}(X, Y) = \frac{E[(X - \mu_X)(Y - \mu_Y)]}{\mu_X \mu_Y}
(\#eq:FCSCorrelationDuplicate)
\end{equation}
```

So correlation is answering the question: _Do $X$ and $Y$ deviate from their mean at the same time and in the same or opposite direction?_ If there is no pattern in their deviations from mean, then the correlation will be zero. If they deviate from their means in the same direction at the same time, there will be a positive correlation; if they deviate from their means in the opposite direction at the same time, there will be a negative correlation. Correlation is all about measuring whether deviation from mean in two different series is synchronised or not.

Let's create two unrelated intensity traces $X$ and $Y$, both of length 500. $X$ will be Poisson($\lambda = 50$) and $Y$ will be Poisson($\lambda = 30$).

(ref:unrelated-intensity-traces-caption) Unrelated intensity traces $X$ and $Y$ with their means in red.

```{r unrelated-intensity-traces, fig.show='hold', out.width='45%', echo=FALSE, fig.height=2.5, fig.width=3, fig.cap='(ref:unrelated-intensity-traces-caption)'}
set.seed(1)
X <- rpois(500, 50)
Y <- rpois(500, 30)
XY <- tibble(X, Y, time = seq_along(X))
ggplot(XY, aes(time, X)) + geom_line() + ylim(0, 75) + 
  geom_hline(yintercept = mean(X), colour = "red")
ggplot(XY, aes(time, Y)) + geom_line() + ylim(0, 75) + 
  geom_hline(yintercept = mean(Y), colour = "red")
bleach_frac <-  0.9
autocor_p_beforebleach <- cor.test(X[-1], dplyr::lag(X)[-1])$p.value %>% 
  round(2)
invisible(assert_that(autocor_p_beforebleach > 0.2))
```

There's no pattern in how $X$ and $Y$ deviate from their means, indeed a Pearson correlation test of $X$ and $Y$ returns an insignificant $p$-value of `r round(cor.test(X, Y)$p.value, 2)`.

Now let's bleach them both by `r round(100 * bleach_frac)`%.

```{r bleached-unrelated-intensity-traces, fig.show='hold', out.width='45%', echo=FALSE, fig.height=2.5, fig.width=3, echo=FALSE, fig.cap='(ref:unrelated-intensity-traces-caption)'}
set.seed(1)
bleach_rate <- log(1 - bleach_frac) / length(X)
X %<>% {. * exp(bleach_rate * seq_along(.))}
Y %<>% {. * exp(bleach_rate * seq_along(.))}
XY <- tibble(X, Y, time = seq_along(X))
ggplot(XY, aes(time, X)) + geom_line() + ylim(0, 75) + 
  geom_hline(yintercept = mean(X), colour = "red")
ggplot(XY, aes(time, Y)) + geom_line() + ylim(0, 75) + 
  geom_hline(yintercept = mean(Y), colour = "red")
invisible(assert_that(cor.test(X, Y)$p.value < 10 ^ (-200)))
autocor_p_afterbleach <- cor.test(X[-1], dplyr::lag(X)[-1])$p.value
invisible(assert_that(autocor_p_afterbleach < 10 ^ (-200)))
```

Now, there is visible correlation. At the start of $X$ and $Y$, both are consistently above their mean and at the end, both are consistently below. They are simultaneously above and below respectively, so there will be positive correlation. This is backed up by a Pearson correlation test which returns a significant $p$-value of less than $10^{-200}$.

(ref:bleaching-introduces-autocorr-remark) Bleaching also introduces autocorrelation for the same reason. The series $X$ had no autocorrelation before bleaching ($p$ = `r autocor_p_beforebleach`), whereas after bleaching, $X$ has significant autocorrelation ($p < 10^{-200}$).

```{remark bleaching-introduces-autocorr}
(ref:bleaching-introduces-autocorr-remark)
```


### FFS

```{definition, name='median mean pixel intensity'}
The _median mean pixel intensity_ of an image series is found by taking the mean intensity of each pixel in the image series and then taking the median of those means. It can be thought of as a summary statistic for the pixel intensity of the image series.
```

```{definition, name='median pixel intensity variance'}
The _median pixel intensity variance_ of an image series is found by taking the variance in the intensity of each pixel in the image series and then taking the median of those variances. It can be thought of as a summary statistic for the variance in the pixel intensity of the image series.
```

In FFS, one is always interested in the mean and variance of pixel values. `img1` has a median mean pixel intensity of `r median(mean_pillars(img1)) %>% round()` and a median pixel intensity variance of `r median(var_pillars(img1)) %>% round()`. The mean brightness is $\epsilon = `r mean(brightness(img1, "e")) %>% round(4)`$ (very close to 4, as expected since the image series was simulated with brightness $\epsilon = 4$). For `img1_bleached`, we find a median mean pixel intensity of `r median(mean_pillars(img1_bleached)) %>% round()` and a median pixel intensity variance of `r median(var_pillars(img1_bleached)) %>% round()`. The mean brightness is $\epsilon = `r mean(brightness(img1_bleached, "e")) %>% round(4)`$. Hence, bleaching has altered both the means and variances of the pixels, resulting in a change in calculated brightness. The non-stationary mean frame intensity introduced by bleaching decreases the mean but increases the variance. The loss in signal has the effect of slightly decreasing the variance: with Poisson statistics (such as photon-emission), a loss of signal (photons) leads directly to a loss in variance. This is a subtle point not discussed anywhere in the literature; it is shown in figure \@ref(fig:visualize-variance-decrease).

(ref:visualize-variance-decrease-caption) A decrease in the Poisson rate (e.g. for emission of photons) leads to a decrease in the mean (blue line) but also a decrease in fluctuations around the mean. Notice that towards the right where rate is low, fluctuations around the mean are at their smallest.

```{r visualize-variance-decrease, fig.asp=0.6, out.width='90%', fig.cap='(ref:visualize-variance-decrease-caption)', echo=FALSE}
df_var_dec_vis <- tibble(rate = seq(100, 0, length.out = 300), 
                         value = detrendr:::myrpois(rate))
ggplot(df_var_dec_vis, aes(rate, value)) + scale_x_reverse() +
  geom_segment(x = -100, y = 100, xend = 0, yend = 0, 
               colour = gg_color_hue(2)[1]) +
  geom_line(colour = gg_color_hue(2)[2]) 
```


## Exponential fitting detrending {#exponential-fitting-detrending}

Naively, one could assume that bleaching of fluorophores takes place at some constant rate. This would mean that the intensity of the image would die-off according to an exponential decay. In figure \@ref(fig:exp-fit-bleach), we fit an exponential decay to such ideal data. 

(ref:exp-fit-bleach-caption) Exponential fit to intensity trace of image which is subject to bleaching at a constant rate.

```{r exp-fit-bleach, fig.cap='(ref:exp-fit-bleach-caption)', echo=FALSE}
set.seed(1)
means <- 100
bleach_fraction_to_rate <- function(frac, n_frames) 1 - (1 - frac) ^ (1 / n_frames)
bleach_rate <- bleach_fraction_to_rate(0.2, 500)
for (i in 1:499) means %<>% c((1 - bleach_rate) * last(.))
devs <- detrendr:::myrpois(means * (1 - 0.9))
df_exp_fit_bleach <- tibble(
  intensity_value = means * 0.9 + devs,
  time = seq_along(intensity_value),
  exponential = means) %>% 
  gather(line, intensity_value, c(intensity_value, exponential)) %>% 
  mutate(line = if_else(line == "exponential", "exponential fit", 
                        "intensity over time"))
ggplot(df_exp_fit_bleach, aes(time, intensity_value, colour = line)) +
  geom_line() + ylab("intensity value")
```

Having fit the data, one may record the deviations from the fitted line as the _fluctuations_ and replace these fluctuations about a straight line, which is placed at the mean of the original series; for figure \@ref(fig:exp-fit-bleach) above, this mean is `r round(mean(means))`. Figure \@ref(fig:exp-fit-corr) shows the corrected series.

(ref:exp-fit-corr-caption) The blue line from figure \@ref(fig:exp-fit-bleach) has changed to a straight horizontal line cutting the $y$ axis at the mean intensity of the original intensity trace. The fluctuations about the blue line that existed in figure \@ref(fig:exp-fit-bleach) are preserved here. As an example, see the large downward fluctuation at $t \approx 50$ seconds in both figures.

```{r exp-fit-corr, fig.cap='(ref:exp-fit-corr-caption)', echo=FALSE}
df_exp_fit_corr <- df_exp_fit_bleach %>% 
  mutate(mean = round(mean(means)), 
         intensity_value = mean + intensity_value - means) %>% 
  select(time, intensity_value, mean) %>% 
  gather(line, intensity_value, c(intensity_value, mean)) %>% 
  mutate(line = if_else(line == "mean", "mean intensity", 
                        "intensity over time"))
ggplot(df_exp_fit_corr, aes(time, intensity_value, colour = line)) +
  geom_line() + ylab("intensity value") + 
  scale_color_manual(values = rev(gg_color_hue(2)))
```

We can see that here, in the ideal case where the naive assumptions of the exponential decay fitting approach hold, this approach works quite well. Let us now examine the case where these assumptions don't hold because there are other long-term fluctuations e.g. due to cell movement. We add these other fluctuations as a gentle sinusoid. See figure \@ref(fig:exp-decay-with-sin).

(ref:exp-decay-with-sin-caption) An exponential decay with added sinusoidal variance, fit with a simple exponential decay.

```{r exp-decay-with-sin, fig.cap='(ref:exp-decay-with-sin-caption)', echo=FALSE}
sin_means <- means %>% {. + 5 * sin(seq_along(.) / 50)}
df_exp_fit_bleach_sin <- tibble(
  intensity_value = sin_means * 0.9 + devs,
  time = seq_along(intensity_value),
  exponential = means) %>% 
  gather(line, intensity_value, c(intensity_value, exponential)) %>% 
  mutate(line = if_else(line == "exponential", "exponential fit", 
                        "intensity over time") %>% 
           factor(levels = c("exponential fit", "intensity over time")))
ggplot(df_exp_fit_bleach_sin, aes(time, intensity_value, colour = line)) +
  geom_line() + ylab("intensity value")
```

One can see by eye that this is not a good fit for the data. This has disastrous consequences for the detrended series, shown in figure \@ref(fig:exp-fit-decay-with-sin-corr).

(ref:exp-fit-decay-with-sin-corr-caption) Result of exponential fitting detrending applied to a decay with a long-term sinusoidal trend component.

```{r exp-fit-decay-with-sin-corr, fig.cap='(ref:exp-fit-decay-with-sin-corr-caption)', echo=FALSE}
df_exp_fit_bleach_sin_corr <- df_exp_fit_bleach_sin %>% 
  mutate(mean = 90, 
         intensity_value = mean + intensity_value - means) %>% 
  select(time, intensity_value, mean) %>% 
  gather(line, intensity_value, c(intensity_value, mean)) %>% 
  mutate(line = if_else(line == "mean", "mean intensity", 
                        "intensity over time"))
ggplot(df_exp_fit_bleach_sin_corr, aes(time, intensity_value, colour = line)) +
  geom_line() + ylab("intensity value") + 
  scale_color_manual(values = rev(gg_color_hue(2)))
```

One can see in figure \@ref(fig:exp-fit-decay-with-sin-corr) that the exponential fit detrend failed to remove the sinusoidal trend in the data (even though it _did_ remove the exponential decay component). We have now seen that exponential fitting detrending is appropriate when the decay has a particular form, but is otherwise not fit for use. This is a problem common to all fitting approaches to detrending, even the more flexible types like polynomial detrending [@polynomial]. For this reason alone, for the purpose of detrending, fitting approaches should be avoided.


## Boxcar smoothing detrending

A common approach to obtaining the line from which to measure deviations/fluctuations (as in the red line in figure \@ref(fig:exp-fit-bleach)) is to _smooth_ the time series, i.e. construct the line by taking a _local average_ at each point. This is often referred to as _boxcar smoothing_ because it can be visualized as drawing a box around a neighborhood of points, taking their average as the smoothed value at that point and then moving the box onto the next series of points and repeating the procedure; see figure \@ref(fig:boxcar-explanation).

(ref:boxcar-explanation-caption) The original time series is depicted by the red dots. The blue rectangles represent the _boxcar_. This boxcar is said to be of length 3 because it is wide enough to encompass 3 points at a time. The boxcar is centered on a point and then the smoothed value at that point (blue dot) is calculated as the mean value of all points within the boxcar. In reality, every point gets a smoothed value which means that the boxcar _overlaps_ but in this figure---for the sake of clarity---they are not overlapped.

```{r boxcar-explanation, fig.cap='(ref:boxcar-explanation-caption)', echo=FALSE}  
set.seed(9)
boxcar_explain_df <- tibble(y = seq(0, 2, length.out = 9) + runif(9),
                            t = seq_along(y))
boxcar_means_df <- tibble(t = roll_mean(boxcar_explain_df$t, n = 3, by = 3), 
                          y = roll_mean(boxcar_explain_df$y, n = 3, by = 3))
ggplot(boxcar_explain_df, aes(t, y)) + 
  geom_point(colour = gg_color_hue(2)[1], size = 2) +
  geom_point(data = boxcar_means_df, aes(t, y), 
             colour = gg_color_hue(2)[2], size = 3) + 
  geom_rect(xmin = 1 - 0.2, xmax = 3 + 0.2, 
            ymin = boxcar_means_df$y[1] - 0.5,
            ymax = boxcar_means_df$y[1] + 0.5,
            colour = gg_color_hue(2)[2], alpha = 0) + 
  geom_rect(xmin = 4 - 0.2, xmax = 6 + 0.2, 
            ymin = boxcar_means_df$y[2] - 0.5,
            ymax = boxcar_means_df$y[2] + 0.5,
            colour = gg_color_hue(2)[2], alpha = 0) + 
  geom_rect(xmin = 7 - 0.2, xmax = 9 + 0.2, 
            ymin = boxcar_means_df$y[3] - 0.5,
            ymax = boxcar_means_df$y[3] + 0.5,
            colour = gg_color_hue(2)[2], alpha = 0) + 
  ylim(-0.3, 3) + xlab("time")
```

The length of the boxcar is equal to $2l + 1$ for natural numbered parameter $l \in \mathbb{N}$. This ensures that the length of the boxcar is always odd) which means it can always be centered upon a point). Hence the allowable lengths of a boxcar are $3,5,7,9,$ etc. 

The boxcar parameter $l$ has a large effect on the type of smoothing achieved. This can be seen in figure \@ref(fig:boxcar-sizes) where boxcar smoothing is applied to the trace in figure \@ref(fig:exp-fit-bleach). The traces for $l=1$ and $l=3$ are far too wiggly (not smooth enough); the trace for $l=75$ is better but perhaps still slightly wiggly; finally, the traces for $l=300$ and $l=1000$ are too close to straight horizontal lines (too smooth). 

(ref:boxcar-sizes-caption) The original intensity trace is shown in the top-left. The other panels show the result of boxcar smoothing for $l = 1, 3, 75, 300$ and $1000$. $l=1$ and $l=3$ are not smooth enough, $l = 75$ looks like it might be OK although it is slightly wiggly. $l = 300$ and $l = 1000$ are over-smoothed.

```{r boxcar-sizes, fig.cap='(ref:boxcar-sizes-caption)', echo=FALSE}
df_exp_fit_bleach %>% 
  spread(line, intensity_value) %>% 
  clean_names() %>% 
  select(-exponential_fit) %>% 
  mutate(boxcar1 = detrendr:::boxcar_smooth(intensity_over_time, 1),
         boxcar3 = detrendr:::boxcar_smooth(intensity_over_time, 3),
         boxcar75 = detrendr:::boxcar_smooth(intensity_over_time, 75),
         boxcar300 = detrendr:::boxcar_smooth(intensity_over_time, 300),
         boxcar1000 = detrendr:::boxcar_smooth(intensity_over_time, 1000)) %>% 
  gather(line, intensity, -time) %>% 
  arrange(first_number(line) %T>% {.[is.na(.)] <- min(., na.rm = TRUE) - 1}) %>%
  mutate(line = str_replace_all(line, "_", " ") %>% 
           str_replace("boxcar", "boxcar l=")) %>% 
  mutate(line = as_factor(line)) %>% 
  ggplot() + aes(time, intensity) + geom_line() +
  facet_wrap(~ line, nrow = 2)
```

This begs the question: what is the correct smoothing parameter $l$? 


## The correct smoothing parameter for detrending

Figure \@ref(fig:boxcar-sizes) shows that the choice of boxcar size is crucial because different sizes lead to very different _smoothed_ lines. The most common choice in the community is to choose $l = 10$ [@SimFCS]. There is no justification for this choice.

In section \@ref(number-and-brightness), we learned that for immobile particles, the expected brightness is $B = 1$. This fact can be used to solve for the appropriate choice of $l$ to use for detrending a specific image series. 

```{definition}
The _mean intensity profile_ of one channel of an image series is obtained by calculating the mean intensity of each frame in that image series.
```

The mean intensity profile can be used to visualize the bleaching of an image series. If the fluorophores are bleaching, the mean intensity should be decreasing over time. To proceed with solving for the appropriate $l$, we need to make one assumption; this is that any two image series with the same mean intensity profile are appropriately detrended with the same detrending parameter $l$. This assumption seems reasonable, however there is no need to debate its validity because later, detrending with the solved-for parameter $l$ will be evaluated with simulated data and compared to the standard $l = 10$. If this assumption is bad, then the performance of the detrending that relies on it should also be bad. With this assumption in hand, solving for $l$ proceeds as follows:

1. Simulate an image series with immobile particles only which has the same mean intensity profile as the acquired real data.
1. Given that the simulated series is of immobile particles only, once properly detrended, it should have $B = 1$.
1. The $l$ for which the detrended series has mean brightness closest to 1 is the most appropriate for the simulated data.
1. By the assumption above, this $l$ is the most appropriate for the real data.

Mathematically, this can be expressed as

\begin{equation}
l = \text{argmin}_{\tilde{l}} |1 - (\text{mean brightness of simulated series detrended with parameter } \tilde{l})|
(\#eq:leq)
\end{equation}

In fact, what I have done here is to give a general method for solving for any detrending parameter $\alpha$:

\begin{equation}
\alpha = \text{argmin}_{\tilde{\alpha}} |1 - (\text{mean brightness of simulated series detrended with parameter } \tilde{\alpha})|
(\#eq:detrend-param)
\end{equation}

This will be useful later when other detrending regimes with their own parameters are introduced.


## Exponential smoothing detrending {#exponential-smoothing-detrending}

_Exponential smoothing_ is a slight alteration to boxcar smoothing. The idea is that when computing a local average, points nearer to the point of interest should have greater weights. The weights fall off with distance $|t|$ from the point of interest according to $\exp(-\frac{|t|}{\tau})$ where the parameter $\tau$ is a positive real number. This function is visualized in figure \@ref(fig:exp-smooth-func). For small values of $\tau$, only values very close to the point of interest have importance when calculating the local average. For larger values of $\tau$, further values also have importance (but closer values always have higher weights). In this sense, increasing the value of $\tau$ has a similar effect to increasing the value of $l$ for the boxcar in that further away points are taken into account. 

(ref:exp-smooth-func-caption) The function $\exp(-\frac{|t|}{\tau})$ visualized with $\tau = 2$ and $\tau = 9$. For $\tau = 2$, points at distance $|t| = 10$ have approximately zero weight, whereas for $\tau = 9$, these points have significant weight.


```{r exp-smooth-func, echo=FALSE, fig.cap='(ref:exp-smooth-func-caption)'}
exp_smooth_func <- function(t, tau) exp(-abs(t / tau))
ggplot(tibble(x = 0, y = x)) + aes(x) +
  stat_function(fun = exp_smooth_func, args = list(tau = 2), 
                aes(colour = "red")) + 
  stat_function(fun = exp_smooth_func, args = list(tau = 9), 
                aes(colour = "blue")) +
  xlim(-30, 30) + 
  ylab(TeX("$\\exp\\left(-\\frac{|t|}{\\tau}\\right)$")) + xlab(TeX("$t$")) +
  scale_colour_manual("", values = c(red = "red", blue = "blue"),
                      labels = unname(TeX(c("$\\tau = 9$", "$\\tau = 2$")))) 
```

In figure \@ref(fig:tau-values), exponential smoothing with different parameters $\tau$ is applied to the trace in figure \@ref(fig:exp-fit-bleach). The results are similar to those in figure \@ref(fig:boxcar-sizes). 

(ref:tau-values-caption) The original intensity trace is shown in the top-left. The other panels show the result of exponential smoothing for $\tau = 1, 3, 75, 300$ and $1000$. 

```{r tau-values, fig.cap='(ref:tau-values-caption)', echo=FALSE}
tau_values_df_rds_path <- here("misc", "tau_values_df.RDS")
if (file.exists(tau_values_df_rds_path)) {
  tau_values_df <- readRDS(tau_values_df_rds_path)
} else {
  tau_values_df <- df_exp_fit_bleach %>% 
  spread(line, intensity_value) %>% 
  clean_names() %>% 
  select(-exponential_fit) %>% 
  mutate(tau1 = detrendr:::exp_smooth(intensity_over_time, 1, 999),
         tau3 = detrendr:::exp_smooth(intensity_over_time, 3, 999),
         tau75 = detrendr:::exp_smooth(intensity_over_time, 75, 999),
         tau300 = detrendr:::exp_smooth(intensity_over_time, 300, 999),
         tau1000 = detrendr:::exp_smooth(intensity_over_time, 1000, 999)) %>% 
  gather(line, intensity, -time) %>% 
  arrange(first_number(line) %T>% {.[is.na(.)] <- min(., na.rm = TRUE) - 1}) %>%
  mutate(line = str_replace_all(line, "_", " ") %>% 
           str_replace("tau", "tau = ") %>% 
           if_else(str_detect(., "="), str_c("$\\", ., "$"), .) %>% 
           TeX("char")) %>% 
  mutate(line = as_factor(line))
  saveRDS(tau_values_df, tau_values_df_rds_path)
}
ggplot(tau_values_df) + aes(time, intensity) + geom_line() +
  facet_wrap(~ line, nrow = 2, labeller = label_parsed)
```

Heuristically, exponential smoothing detrending seems favorable to boxcar detrending because the idea that points further away from the point of interest are less important (but still somewhat important) when computing the local average is reasonable. Indeed, this was the method proposed in the original number and brightness paper [@NB]. For this reason, exponential smoothing was the method of choice for my paper where the method of choosing the correct detrending parameter was published [@nandb].  


## Correcting for non-stationary variance {#correcting-for-non-stationary-variance}

```{definition}
The variance of a random variable $X$ is the expected value of the squared deviation of $X$ from its mean $\mu$:
\begin{equation}
\text{Var}(X) = E[(X - \mu)^2]
(\#eq:variance)
\end{equation}
```

All of this chapter so far has focused on correcting for non-stationary mean. As shown in figure \@ref(fig:visualize-variance-decrease), as the mean decreases, so too does the variance. For an instance $x$ of the random variable $X$ with expected value $E[X] = \mu$, $x - \mu$ is the _deviation_ of $x$ from $\mu$. If we write $x$ as $x = \mu + \tilde{x}$, then we get the deviation $x - \mu = (\mu + \tilde{x}) - \mu = \tilde{x}$, so $\tilde{x}$ is the deviation. For a given point in figure \@ref(fig:visualize-variance-decrease), its deviation is its distance from the red line. For positive real number $k$, making the transformation $\tilde{x} \rightarrow \sqrt{k}\tilde{x}$ i.e. $x \rightarrow \mu + \sqrt{k}\tilde{x}$ causes the variance (i.e. the _squared deviation_) to be transformed as $\text{Var}(X) \rightarrow k \times \text{Var}(X)$. Hence, we have a way to modify the variance of a time series as a whole by modifying the deviation of each time point from the mean. For months, I toyed with this idea as a solution of correcting for non-stationary variance. However, in reality the contribution to the variance in intensity at a given pixel is down to both Poisson photon statistics and fluorophore movement. This combination of factors makes it very difficult to ascertain the amount by which the variance should be altered. I eventually abandoned my efforts to alter the variance like this in favor of the _Robin Hood_ detrending algorithm (section \@ref(robin-hood-detrending)) which includes correction for non-stationary variance as an intrinsic part of its detrending routine.


## Caveats of fitting and smoothing approaches

Both fitting and smoothing approaches to detrending have serious caveats. Fitting approaches assume that the fluorescence intensity decay has a certain form. Unpredictable issues such as cell movement mean that no particular decay form can be assumed. Smoothing methods do not perform well at the edges of time series that they are applied to. They also require the user to choose a smoothing parameter. The problem of how to best choose this parameter was solved recently [@nandb], but this method has not been widely adopted. Most importantly, both fitting and smoothing fail when the data cannot be approximated as mathematically continuous (fitted and smoothed lines are continuous approximations of data). Fluorescence intensity data at low intensities---where most pixel values are either 0 or 1---are quasi-binary^[By _quasi-binary_, I just mean that almost all values are 0 or 1.] and hence a continuous approximation does not make sense (see figure \@ref(fig:fit-smooth-caveats)). This means that neither fitting nor smoothing are applicable detrending methods at low intensities. This is the crucial caveat of these methods because, when bleaching is a problem, it is common to reduce laser power to reduce bleaching, which leads directly to lower intensity images. With fitting and smoothing techniques, it may sometimes be advisable to increase the laser power to achieve higher intensities such that the detrending routines will function properly. This means one may need to bleach more in order to be able to correct for bleaching. This farcical situation necessitates a new detrending technique which can function at low intensities. 

(ref:fit-smooth-caveats-caption) Left: for high ($\gg 1$) intensity values, the line is a satisfactory approximation of the data, representing it well. Right: for low (quasi-binary) intensity values, the line is not a good approximation for the data and indeed no line or curve could represent the data well.

```{r fit-smooth-caveats, fig.width=3.5, fig.show='hold', out.width='49%', fig.cap='(ref:fit-smooth-caveats-caption)', echo = FALSE, fig.asp = 0.7}
set.seed(1)
tibble(t = seq_len(100), y = detrendr:::myrpois(seq(100, 50, length = 100))) %>% 
  ggplot() + aes(t, y) + geom_point(colour = gg_color_hue(2)[2]) + 
  geom_segment(x = 1, xend = 100, y = 100, yend = 50, 
               colour = gg_color_hue(2)[1]) + ylab("Intensity") + xlab("time")
tibble(t = seq_len(100), y = detrendr:::myrpois(seq(0.5, 0, length = 100))) %>% 
  ggplot() + aes(t, y) + geom_point(colour = gg_color_hue(2)[2]) + 
  geom_segment(x = 1, xend = 100, y = 0.5, yend = 0, 
               colour = gg_color_hue(2)[1]) + ylab("Intensity") + xlab("time")
```


## Robin Hood detrending {#robin-hood-detrending}

Intensity images in units of photons are count data. This means that the values are all natural numbers, i.e. elements of $\mathbb{N}_0=\{0, 1, 2, 3, \ldots\}$. Fitting and smoothing give real-numbered values (elements of $\mathbb{R}$), which must then be transformed back into count data (elements of $\mathbb{N}_0$), normally by rounding. This means that fitting and smoothing methods of detrending push values through $\mathbb{N}_0 \rightarrow \mathbb{R} \rightarrow \mathbb{N}_0$ (the move back $\rightarrow \mathbb{N}_0$ is necessary because calculations like Qian and Elson's moment analysis assume photon count data, which is necessarily in $\mathbb{N}_0$). When current methods were failing to properly detrend low-intensity images, I began to wonder was it necessary to go through the real numbers $\mathbb{R}$, given that the start and end points were the natural numbers $\mathbb{N}_0$?

Consider figure \@ref(fig:rh-detrend-eg). There is a bleached and unbleached version of an intensity trace. Suppose that our real data is the bleached trace, but we _wish_ it looked like the unbleached trace. You may wonder why the unbleached trace is not at the starting intensity of the bleached series. For reasons that will become clear, the Robin Hood algorithm can only place the detrended image at the mean intensity of the original image. This is not a problem because the issue with bleaching in FCS and FFS is mainly that the changing signal leads to incorrect calculations, not that the loss in signal leads to a critical lack of information (photons). Indeed, a feature of the Robin Hood algorithm is that it preserves the mean intensity of the real data on a pixel-by pixel basis.

(ref:rh-detrend-eg-caption) Bleached and unbleached intensity traces.

```{r rh-detrend-eg, fig.cap='(ref:rh-detrend-eg-caption)', echo=FALSE}
set.seed(1)
rh_detrend_eg_df <- tibble(
  x = seq(100, 80, length.out = 100),
  devs = runif(length(x), -2, 2),
  bleached = x + devs,
  unbleached = mean(x) + devs,
  time = seq_along(x)) %>% 
  gather(line, Intensity, bleached:unbleached)
ggplot(rh_detrend_eg_df, aes(time, Intensity, colour = line)) + geom_line()
```

To get to the unbleached intensity trace from the bleached intensity trace, intensity must be subtracted from time-points with too much intensity and added to time points with too little intensity. This can be done by _taking_ counts from frames with too much intensity and _giving_ them to frames with too little intensity. In this way, no counts are gained or lost, they are just moved around the image series. See figure \@ref(fig:rh-lego). Counts are passed from one frame to another _along_ a given pixel, i.e. if a count is taken from pixel at position $p$ in some frame $i$, it must be given to a pixel at the same position $p$ in some other frame $j$. It is this condition that ensures that the mean intensity images of the original and detrended image series are the same. 

(ref:rh-lego-caption) Robin Hood: counts are taken from frames of higher intensity (usually closer to the start of the image series) and given to frames of lower intensity (usually closer to the end of the image series).

```{r rh-lego, fig.show='hold', out.width='48%', echo=FALSE, fig.asp=0.4, fig.cap='(ref:rh-lego-caption)'}
rh_lego_gg <- filter(rh_detrend_eg_df, line == "bleached") %>% 
  ggplot() + aes(x = time, y = Intensity) + geom_line(colour = gg_color_hue(2)[1]) +
  geom_hline(yintercept = 90, linetype = "dotted") +
  geom_curve(aes(x = 4, xend = 85, y = 97.5, yend = 81), 
             arrow = arrow(), curvature = 0.3)
ggsave(filename = here("img", "rh_lego_plot.png"), plot = rh_lego_gg, 
       width = 4, height = 3.7)
ggsave(filename = here("img", "rh_lego_plot.pdf"), plot = rh_lego_gg, 
       width = 4, height = 3.7)
include_graphics(here("img", rev(c("rh_lego_plot.png", "rh.png"))))
```

To determine how many swaps need to be made to detrend a given image series, equation \@ref(eq:detrend-param) can be used, with $\alpha$ being the number of swaps.

The random gifting of counts from higher to lower intensity frames has the effect of temporally redistributing mean intensity but _also_ variance in intensity. With photon statistics (which follow a Poisson distribution), random counts provide both mean and variance. This is in contrast to all previous methods which consist of determining local deviation and adding it to a _fixed_ global mean: this provides no temporal redistribution of variance.


## A comparison of detrending methods {#detrend-compare}

To compare the various detrending methods, I use the following workflow:

1. Simulate a number $N = 100,000$ of particles diffusing with known diffusion rate. Simulations were done with the `brownded` software package (section \@ref(brownded)).
1. Simulate photon emission from these particles with chosen brightness $\epsilon$ and create an image series from this, being careful to (virtually) sample at a rate appropriate for number and brightness analysis.
1. Bleach the simulation with a chosen constant bleaching rate.^[It would be good to test with non-constant bleaching rates too, this wasn't done due to time constraints.]
1. Simulate photon emission from the bleached simulation (bleached particles don't emit photons) with the same brightness $\epsilon$ and create an image series. 
1. Detrend the bleached image series.
1. Evaluate the detrending algorithm by measuring how close the brightness of the detrended bleached image series is to the known simulated brightness.

For all combinations of brightnesses of $\epsilon = 0.001, 0.01, 0.1, 1, 10$ and bleaching fractions of 0%, 1%, 5%, 10%, 15%, 20%, 25%, 30%, 20 images of 64x64 pixels and 5,000 frames were simulated using 100,000 fluorescent diffusing particles.^[The simulation took 3 weeks.] These were detrended with the following detrending routines:^[The detrending took 2 weeks.]

1. Boxcar with $l = 10$ (`boxcar10`, the most common detrending routine).
1. Exponential smoothing with automatically chosen parameter $\tau$ (`autotau`).
1. Robin Hood with automatically chosen swaps (`robinhood`).

The performance was evaluated using the _mean relative error_. 

```{definition}

For a given brightness and bleaching fraction,

\begin{equation}
\text{mean relative error } = \frac{|(\text{calculated brightness after detrending}) - (\text{true brightness})|}{(\text{true brightness})}
(\#eq:mean-relative-error)
\end{equation}

```

Figure \@ref(fig:detrend-compare) shows the results. Before I discuss them, note that the common brightnesses that we see are in the range $\epsilon = 0.003$ to $\epsilon = 0.1$. 

(ref:detrend-compare-caption) A comparison of different detrending methods with various brightnesses and bleaching fractions (steady, constant-rate bleaching), including the results of not detrending at all.

```{r detrend-compare, echo=FALSE, fig.cap='(ref:detrend-compare-caption)'}
include_graphics(here("img", "detrend_compare.png"))
```

The most striking thing about figure \@ref(fig:detrend-compare) is that the best choice in all cases is to not detrend at all! This is an interesting result and seems to render all detrending routines worthless. However, when working with real data, not detrending does not work well at all. This will be shown in chapter \@ref(applications). This indicated that my simulations are unrealistic. This is probably because with real data, bleaching is likely not taking place at a constant, steady rate and other factors such as cell movement or/and laser power fluctuations are contributing to medium and long term intensity fluctuations and these have a detrimental effect on calculations if not detrended out. It would be possible to study this by mimicking real bleaching profiles with simulations (see section \@ref(mimic)). 

The worst performer by far is `boxcar10`. For example, at $\epsilon = 0.1$, it makes an error of worse than 40% and for $\epsilon = 0.001, 0.01$, its error is worse than 50%, so it does not even appear on the plot. This is good evidence that arbitrarily choosing the parameter $l$ is very bad practice. For realistic brightnesses ($\le 0.1$), `robinhood` is the best with errors almost always lower than 5%. `autotau` also performs very well, with errors almost always less than 10%. At the lowest brightness $\epsilon = 0.001$, all methods are somewhat erratic. That is because at this extremely low brightness, there is a critical lack of information (photons) for the algorithms to work with. Finally, at unrealistically high brightnesses of $\epsilon = 1, 10$, `autotau` begins to perform well because at these high photon counts, the caveats of smoothing have totally disappeared. However, I cannot explain the degradation in the performance of `robinhood` in this case. Fortunately, there is no need to dwell on this, as this situation ($\epsilon = 1, 10$) does not arise in practice because available fluorophores are not this bright.