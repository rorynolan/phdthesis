[
["index.html", "Algorithms for the Correction of Photobleaching Prerequisites", " Algorithms for the Correction of Photobleaching Rory Nolan 2018-05-29 Prerequisites "],
["intro.html", "Chapter 1 Introduction 1.1 Confocal light microscopy 1.2 Intensity traces 1.3 Fluorescence fluctuation spectroscopy 1.4 Fluorescence correlation spectroscopy", " Chapter 1 Introduction 1.1 Confocal light microscopy All of the images used in my PhD were collected on a confocal microscope. This type of microscope guarantees that only in-focus light is collected at the detector. See figure 1.1. Figure 1.1: Confocal microscope light path showing how out of focus light does not make it to the detector.1 Definition 1.1 The confocal volume is the in-focus volume within a sample that is efficiently detected using a system designed with confocal optics.2 An image is acquired on a confocal microscope by scanning this apparatus across a sample, collecting one pixel at a time. Some confocal detectors collect intensity in arbitrary units but others are capable of photon-counting. The work in this thesis was carried out with photon-counting detectors. Remark. Henceforth, intensity counts will be assumed to be in units of photons. 1.2 Intensity traces Definition 1.2 A fluorophore is a fluorescent chemical compound that can re-emit light upon light excitation.3 Fluorophores under constant excitation emit light one photon at a time according to a poisson statistics4 with some poisson rate. Consider figure 1.2. A fluorophore enters the confocal volume, is excited there and emits photons which are collected by the detector. When the fluorophore is not in the confocal volume (start and end), no photons are detected. When the particle is in the confocal volume, photons it emits are collected at the detector. Different numbers of photons are collected per unit time (per ms here). Figure 1.2: Left: a fluorophore diffusing trhough the confocal volume. Right: the intensity trace due to this fluorophore. Definition 1.3 The time-series of intensity counts in a given pixel or image is referred to as its intensity trace. 1.3 Fluorescence fluctuation spectroscopy Broadly, fluorescence fluctuation spectroscopy (FFS) is the analysis of the intensity fluctuation of a fluorescence signal (Chen et al. 1999). This very often takes the form of moment analysis5 (Qian and Elson 1990). Briefly, moment analysis is an attempt to extract data from a distribution of values using its moments. The first moment of a distribution is its mean value, the second moment is its variance and so on. Intensity traces can be viewed as distributions with moments. For example, the intensity trace in figure 1.2 has mean 0.27 and variance 0.48. 1.3.1 Number and brightness Number and brightness (N&amp;B, Digman et al. (2008)) is an FFS technique for quantifying the oligomeric states of fluorescently labelled proteins. What follows is a mathematical description of the technique. Definition 1.4 An entity is a set of molecules which are chemically bound. Definition 1.5 The brightness \\(\\epsilon\\) of an entity is the mean number of photon detector counts it gives per unit time when in the illumination (confocal) volume. For an image series where the \\(i\\)th slice in the stack is the image acquired at time \\(t = i\\), for a given pixel position \\((x, y)\\), we define \\(\\langle I \\rangle\\) as the mean intensity of that pixel over the image series and \\(\\sigma^2\\) as the variance in that intensity. Define \\(n\\) as the mean number of entities in the illumination volume corresponding to that pixel. Assuming that all entities are mobile, we have \\[\\begin{align} N &amp;= \\frac{\\langle I \\rangle^2}{\\sigma^2} = \\frac{\\epsilon n}{1 + \\epsilon} \\tag{1.1} \\\\ B &amp;= \\frac{\\sigma^2}{\\langle I \\rangle} = 1 + \\epsilon \\tag{1.2} \\end{align}\\] where \\(N\\) and \\(B\\) are referred to as the “apparent number” and “apparent brightness” respectively. This gives \\[\\begin{align} \\epsilon &amp;= \\frac{\\sigma^2}{\\langle I \\rangle} - 1 \\tag{1.3} \\\\ n &amp;= \\frac{\\langle I \\rangle^2}{\\sigma^2 - \\langle I \\rangle} \\tag{1.4} \\end{align}\\] — Nolan, Iliopoulou, et al. (2017)6 The quantity \\(\\epsilon\\) is a relative measure of oligomeric state. That is, \\(\\epsilon\\) will be twice as big for dimers as it is for monomers, three times as big for trimers as it is for monomers and so on. The way that N&amp;B experiments to determine unknown oligomeric states are generally done is as follows: For a given laser power and fluorophore with a system where all entities are known to be monomeric, measure the brightness \\(\\epsilon\\). Call this \\(\\epsilon_\\text{monomer}\\). With the same laser power and fluorophore but now with a system where the oligomeric state is unknown, measure the brightness \\(\\epsilon\\) again. Call this \\(\\epsilon_\\text{unknown}\\). The unknown oligomeric state is equal to \\(\\epsilon_\\text{unknown} / \\epsilon_\\text{monomer}\\). Number and brightness has specific pixel dewll-time and frame rate requirements. These were first articulated in my 2017 review of N&amp;B. Definition 1.6 The pixel dwell time \\(t_\\text{dwell}\\) of a scanning confocal microscope is the amount of time it spends collecting photons at each given pixel before moving on to the next pixel. Definition 1.7 The frame time of a scanning confocal microscope is the amount of time it takes to acquire the data for all of the pixels in a whole frame. That is the length of time from the start of detectionion of photons from the first pixel to the end of detection of photons from the last pixel. Definition 1.8 The mean residence time \\(\\tau_D\\) of a particle in the confocal volume is the average length of time that a particle that enters the confocal volume resides in that volume before exiting. The requirement is \\(t_\\text{dwell} \\ll \\tau_D \\ll t_\\text{frame}\\). This ensures that: When acquiring photons at a given pixel, the underlying configuration of entities is constant (there’s not enough time for the entities to move and change their configuration). When the scanner returns to a given pixel (one frame time later), the underlying configuration has changed totally since the last time the scanner was at this pixel (because so much time has passed, all of the diffusing entities have moved a lot in the meantime). Both of these points are implicitly assumed in the derivation of the N&amp;B equations so it is essential to get these acquisition parameters right. This is discussed at length in Nolan, Iliopoulou, et al. (2017). An important property of N&amp;B is that if all fluorescent particles are immobile, then \\(B = 1\\). This is because photon emission from stationary sources happens according to a poisson distribution. Poisson distributions have variance equal to mean, this implies \\(\\sigma^2\\) = \\(\\langle I \\rangle\\) which gives \\(B = \\frac{\\sigma^2}{\\langle I \\rangle} = 1\\). The N&amp;B technique is fraught with technical difficulties. Principal of these is the problem of photobleaching. Much of my PhD focussed on corrections for photobleaching. This is discussed in chapter 3. 1.4 Fluorescence correlation spectroscopy Fluorescence correlation spectroscopy (FCS) is the correlation analysis of fluorescence intensity fluctuations.7 For this reason, FCS can be described as a subfield of FFS (Jameson, Ross, and Albanesi 2009). In practice, FFS is mostly used to refer to the non-FCS parts of the whole FFS field. I will follow that convention. First, let us introduce some concepts from statistics. Definition 1.9 (statistics) The correlation between two random variables \\(X\\) and \\(Y\\) with expected values \\(\\mu_X\\) and \\(\\mu_Y\\) and standard deviations \\(\\sigma_X\\) and \\(\\sigma_Y\\) is \\[\\begin{equation} \\text{corr}(X, Y) = \\frac{E[(X - \\mu_X)(Y - \\mu_Y)]}{\\sigma_X \\sigma_Y} \\tag{1.5} \\end{equation}\\] where \\(E\\) is the expectation operator.8 Definition 1.10 (statistics) Autocorrelation \\(G(X; \\tau)\\), is the correlation of a signal \\(X\\) with a delayed copy of itself as a function of the delay \\(\\tau\\).9 \\[\\begin{equation} G(X; \\tau) = \\text{corr}(X_t, X_{t + \\tau}) = \\frac{E[(X_t - \\mu_X)(X_{t + \\tau} - \\mu_X)]}{\\sigma^2_X} \\tag{1.6} \\end{equation}\\] Definition 1.11 (statistics) The cross-correlation of two series is the correlation of one with a delayed copy of the other as a function of the delay. \\[\\begin{equation} \\text{crosscorr}(X, Y; \\tau) = \\text{corr}(X_t, Y_{t + \\tau}) = \\frac{E[(X_t - \\mu_X)(Y_{t + \\tau} - \\mu_Y)]}{\\sigma_X \\sigma_Y} \\tag{1.7} \\end{equation}\\] For the purposes of FCS, these quantities were redefined as follows. Definition 1.12 (FCS) The correlation between two random variables \\(X\\) and \\(Y\\) with expected values \\(\\mu_X\\) and \\(\\mu_Y\\) and standard deviations \\(\\sigma_X\\) and \\(\\sigma_Y\\) is \\[\\begin{equation} \\text{corr}(X, Y) = \\frac{E[(X - \\mu_X)(Y - \\mu_Y)]}{\\mu_X \\mu_Y} \\tag{1.8} \\end{equation}\\] where \\(E\\) is the expectation operator.10 Definition 1.13 (FCS) Autocorrelation \\(G(X; \\tau)\\), is the correlation of a signal \\(X\\) with a delayed copy of itself as a function of the delay \\(\\tau\\).11 \\[\\begin{equation} G(X; \\tau) = \\text{corr}(X_t, X_{t + \\tau}) = \\frac{E[(X_t - \\mu_X)(X_{t + \\tau} - \\mu_X)]}{\\mu^2_X} \\tag{1.9} \\end{equation}\\] Definition 1.14 (FCS) The cross-correlation of two series is the correlation of one with a delayed copy of the other as a function of the delay. \\[\\begin{equation} \\text{crosscorr}(X, Y; \\tau) = \\text{corr}(X_t, Y_{t + \\tau}) = \\frac{E[(X_t - \\mu_X)(Y_{t + \\tau} - \\mu_Y)]}{\\mu_X \\mu_Y} \\tag{1.10} \\end{equation}\\] The reason for these redefinitions (which just involve replacing standard deviations with means in the denominators of each expression) is that with the FCS definition, the autocorrelation has the nice property that for normal diffusion \\[\\begin{equation} G(X; 0) = \\frac{1}{n} \\tag{1.11} \\end{equation}\\] where \\(n\\) is the mean number of fluorescent particles in the focal volume. The convenience of the statistics definitions is that there, correlations are guaranteed to be in \\([-1, 1]\\), with \\(0\\) representing no correlation, \\(1\\) perfect positive correlation and \\(-1\\) perfect negative correlation; this is lost with the FCS definitions. I felt it necessary to provide these definitions for two reasons: It is important for people from the fields of FCS and pure mathematics/statistics to know that they have different definitions for the same thing. In FCS, it’s very common for people to mistake correlation for cross-correlation. This is unfortunate but knowing about this common mistake is essential for navigating the field in a sensible manner. It seems that when people in the field correlate the signals from two separate channels, the use the term _cross-_correlation, even though they’re only using correlation. I think the idea of working across two or more channels (and ideas such as cross-talk) leads to this confustion. Henceforth, the FCS defintions of these quantities will be assumed. 1.4.1 Correlation Suppose that two proteins of interest A and B are labelled with red and green fluorophores respectively (and there is no bleed-through between these red and green channels). 1. If these proteins are interacting, then * interaction implies that A and B are stuck together * this implies that A and B co-diffuse * this implies that for a given volumen in the sample - more of A implies more of B - less of A implies less of B - more of B implies more of A - less of B implies less of A Since the number of photons emitted is proportional to the amount of fluorophores present, it follows that * more red photons implies more green photons * less red photons implies less green photons * more green photons implies more red photons * less green photons implies less red photons Altogether, this implies that the intensity traces from the red and green channels will be correlated. 2. If these proteins are not interacting, then the intensity traces from the red and green channels will not be correlated. Thus, interaction of red A and green B necessarily leads to correlation in fluorescent signals from the red and green chanels.12 1.4.2 Autocorrelation As mentioned already, the autocorrelation function (ACF) can be used to count the number of particles in the confocal volume. It can also be used to measure diffusion coefficients for various types of diffusion (normal, anomalous, polydisperse, etc.). The ACF is not used in my PhD. 1.4.3 Cross-correlation The cross-correlation of intensity traces from nearby pixels can be used to measure the velocity of the movement of the labelled particles between these two pixels (Hebert, Costantino, and Wiseman 2005). It can also be used to image barriers to diffusion (Digman and Gratton 2009). 1.4.4 Cross-correlated brightness Cross-correlated brightness (Digman et al. 2009) moulds the correlation idea (section 1.4.1) into the framework of N&amp;B. Suppose there are two channels with intensities \\(I_1\\) and \\(I_2\\). Definition 1.15 (cross-variance) \\[\\begin{equation} \\sigma^2_\\text{cc} = E[(I_2 - \\langle I_1 \\rangle)(I_2 - \\langle I_2 \\rangle)] \\tag{1.12} \\end{equation}\\] Definition 1.16 (cross-correlated brightness) \\[\\begin{equation} B_\\text{cc} = \\frac{\\sigma^2_\\text{cc}}{\\sqrt{\\langle I_1 \\rangle \\langle I_2 \\rangle}} \\tag{1.13} \\end{equation}\\] \\(B_\\text{cc}\\) is related to \\(\\text{corr}(I_1, I_2)\\) by \\[\\begin{equation} B_\\text{cc} = \\sqrt{\\langle I_1 \\rangle \\langle I_2 \\rangle} \\times \\text{corr}(I_1, I_2) \\tag{1.14} \\end{equation}\\] This means that \\(B_\\text{cc}\\) is just a scaled version of correlation. The need for this redefinition is unclear (but it is no harm). It does make the formula look like the brightness formula (1.2), but no such oligomeric state information can be gleaned from \\(B_\\text{cc}\\). It is merely useful as a relative measure of interaction: higher \\(B_\\text{cc}\\) means more interaction, lower \\(B_\\text{cc}\\) means less interaction. It is commonly used to identify interactions. Then, conventional N&amp;B performed on each of the channels (1 and 2) can be used to measure the stoichiometry of the interaction. See Digman et al. (2009) for details. Remark. Since cross-correlated brightness uses correlation but not cross-correlation, it is a prime example of the confusing naming that pervades FCS. It should be called correlated brightness. Rather than rename it, I will continue to refer to it as cross-correlated brightness. References "],
["literature.html", "Chapter 2 Literature", " Chapter 2 Literature Here is a review of existing methods. "],
["photobleaching-correction.html", "Chapter 3 Photobleaching Correction 3.1 Introduction to photobleaching 3.2 The effects of photobleaching in FCS and FFS 3.3 Correcting for photobleaching", " Chapter 3 Photobleaching Correction 3.1 Introduction to photobleaching In the ideal case, an incident photon of appropriate wavelength is absorbed by a fluorophore, promoting the fluorophore to an excited state; subsequently, the fluorophore relaxes down to its ground state by emitting a photon. In reality, it is possible that the incident photon can break the fluorophore with the result that it will no longer emit light. This breaking is referred to as photobleaching (or bleaching for short). Bleaching causes a diminution in the number of effective fluorophores which is a direct cause of a loss of fluorescent signal. Many quantitative methods in fields such as fluorescence fluctuation spectroscopy (FFS) and fluorescence correlation spectroscopy (FCS) implicitly assume that there is no bleaching in the data. Hence, data (image series) with significant levels of photobleaching must be corrected prior to the application of equations and algorithms in these fields. A main focus of this thesis is on how to correct fluorescent image series for the effects of bleaching, given that bleaching does occur. There is no emphasis on understanding why and/or how photobleaching occurs. 3.2 The effects of photobleaching in FCS and FFS We simulate two image series, each with 100,000 diffusing fluorescent particles. In the first image series (img1), these have brightness \\(\\epsilon = 4\\) and in the second (img2) they have brightness \\(\\epsilon = 7\\). See figure 3.1. We bleach these by 15% and 20% to create img1_bleached and img2_bleached respectively. Figure 3.1: Frames 1, 100, 200, 300, 400 and 500 from image series of 100,000 diffusing emitters of brightness \\(\\epsilon = 4\\) (top) and \\(\\epsilon = 7\\) (bottom) with 15% (top) and 20% (bottom) bleaching. It may not be obvious that these image series are subject to bleaching from figure 3.1, but we can see it more clearly in figure 3.2. Figure 3.2: Mean intensity profiles of the simulated image series with and without bleaching. Simulation 1 was bleached by 15% and simulation 2 by 20%. 3.2.1 FCS The unrelated images img1 and img2 have a tiny median cross-correlated brightness of \\(B_\\text{cc} = 0.0036\\), signifying no significant correlation, as one would expect. However, the bleached images img1_bleached and img2_bleached have a significant \\(B_\\text{cc} = 0.3686\\). This shows that bleaching is introducing correlation between otherwise unrelated images. Since correlation is used as a proxy for hetero-interaction, bleaching can make it appear as though there is interaction when in fact there is not when using FCS techniques. 3.2.2 FFS Definition 3.1 (median mean pixel intensity) The median mean pixel intensity of an image series is found by taking the mean intensity of each pixel in the image series and then taking the median of those means. It can be thought of as a summary statistic for the pixel intensity of the image series. Definition 3.2 (median pixel intensity variance) The median pixel intensity variance of an image series is found by taking the variance in the intensity of each pixel in the image series and then taking the median of those variances. It can be thought of as a summary statistic for the variance in the pixel intensity of the image series. In FFS, one is always interested in the mean and variance of pixel values (and sometimes other fluctuation quantities like the third moment). img1 has a median mean pixel intensity of 97.655 and a median pixel intensity variance of 487.2476954. The mean brightness is \\(\\epsilon = 3.9959\\) (very close to 4, as expected since the image series was simulated with brightness \\(\\epsilon = 4\\)). For img1_bleached, we find a median mean pixel intensity of 90.086 and a median pixel intensity variance of 468.4618978. The mean brightness is \\(\\epsilon = 4.2026\\). Hence, bleaching has altered both the means and variances of the pixels, resulting in a change in calculated brightness. The non-stationary mean frame intensity introduced by bleaching decreases the mean but increases the variance. The loss in signal has the effect of slightly decreasing the variance: with poisson statistics (such as photon-emission), a loss of signal (photons) leads directly to a loss in variance. This is a subtle point not discussed anywhere in the literature; it is shown in figure 3.3. Figure 3.3: A decrease in the poisson rate (e.g. for emmission of photons) leads to a decrease in the mean (blue line) but also a decrease in fluctuations around the mean. Notice that towards the right where rate is low, fluctuations around the mean are at their smallest. 3.3 Correcting for photobleaching All of the current literature mentions bleaching correction as being purely for correcting the problem of non-stationary mean (NSM), neglecting the problem of non-stationary variance (NSV). Figure 3.3 shows that NSM and NSV go hand-in-hand. Correction for NSM is referred to as trend removal or detrending. Hence, the terms detrending and bleaching correction have come to be used interchangeably. I will follow this convention and use the term detrending from now on to mean correction for NSM and possibly also NSV. In what follows immediately here, I will focus on correcting for NSM. Discussion of correction for NSV starts in section 3.3.5. 3.3.1 Exponential fitting Naively, one could assume that bleaching of fluorophores takes place at some constant rate. This would mean that the intensity of the image would die-off according to an exponential decay. In figure 3.4, we fit an exponential decay to such ideal data. Figure 3.4: Exponential fit to intensity trace of image which is subject to bleaching at a constant rate. Having fit the data, one may record the deviations from the fitted line as the fluctuations and replace these fluctuations about a straight line, which is placed at the mean of the original series; for figure 3.4 above, this mean is 90. Figure 3.5 shows the corrected series. Figure 3.5: The blue line from figure 3.4 has changed to a straight horizontal line cutting the \\(y\\) axis at the mean intensity of the original intensity trace. The fluctuations about the blue line that existed in figure 3.4 are preserved here. As an example, see the large downward fluctuation at \\(t \\approx 50\\) seconds in both figures. We can see that here, in the ideal case where the naive assumptions of the exponential decay fitting approach hold, this approach indeed does quite well. Let us now examine the case where these assumptions don’t hold because there are other long-term fluctuations e.g. due to cell movement. We add these other fluctuations as a gentle sinusiod. See figure 3.6. Figure 3.6: An exponential decay with added sinusoidal variance, fit with a simple exponential decay. One can see by eye that this is not a good fit for the data. This has disastrous consequences for the detrended series, shown in figure 3.7. Figure 3.7: Result of exponential fitting detrending applied to a decay with a long-term sinusoidal trend component. One can see in figure 3.7 that the exponential fit detrend failed to remove the sinusoidal trend in the data (even though it did remove the exponential decay component). We have now seen that exponential fitting detrending is appropriate when the decay has a particular form, but is otherwise not fit for use. This is a problem common to all fitting approaches to detrending, even the more flexible types like polynomial detrending (Chan, Hayya, and Ord 1977). For this reason alone, for the purpose of detrending, fitting approaches should be avoided. 3.3.2 Boxcar smoothing A common approach to obtaining the line from which to measure deviations/fluctuations (as in the red line in figure 3.4) is to smooth the time series, i.e. construct the line by taking a local average at each point. This is often referred to as boxcar smoothing because it can be visualised as drawing a box around a neighbourhood of points, taking their average as the smoothed value at that point and then moving the box onto the next series of points and repeating the procedure. See figure 3.8. Figure 3.8: The original time series is depicted by the red dots. The blue rectangles represent the boxcar. This boxcar is said to be of length 3 because it is wide enough to encompass 3 points at a time. The boxcar is centred on a point and then the smoothed value at that point (blue dot) is calculated as the mean value of all points within the boxcar. In reality, every point gets a smoothed value which means that the boxcar overlaps but in this figure, for the sake of clarity, they are not overlapped. The paremeter \\(l\\) of a boxcar is such that the length of the boxcar is equal to \\(2l + 1\\). This ensures that the length of the boxcar is always odd) which means it can always be centred upon a point). Hence the allowable lengths of a boxcar are \\(3,5,7,9,\\) etc. The boxcar parameter \\(l\\) has a large effect on the type of smoothing achieved. This can be seen in figure 3.9 where boxcar smoothing is applied to the trace in figure 3.4. The traces for \\(l=1\\) and \\(l=3\\) are far too wiggly (not smooth enough); the trace for \\(l=75\\) is better but perhaps still slightly wiggly; finally, the traces for \\(l=300\\) and \\(l=1000\\) are too close to straight horizontal lines (too smooth). Figure 3.9: The original intensity trace is shown in the top-left. The other panels show the result of boxcar smoothing for \\(l = 1, 3, 75, 300\\) and \\(1000\\). This begs the question: what is the correct smoothing parameter \\(l\\)? 3.3.3 Choosing the correct smoothing parameter for detrending Figure 3.9 shows that the choice of boxcar size is crucial because different sizes lead to very different “smoothed” lines. The most common choice in the community is to choose \\(l = 10\\) (Laboratory for Fluorescence Dynamics 2018). There is no justification for this choice. In section 1.3.1, we learned that for immobile particles, the expected brightness is \\(B = 1\\). This fact can be used to solve for the appropriate choice of \\(l\\) to use for detrending a specific image series. Definition 3.3 The mean intensity profile of one channel of an image series is obtained by calculating the mean intensity of each frame in that image series. The mean intensity profile can be used to visualise the bleaching of an image series. If the fluorophores are bleaching, the mean intensity should be decreasing over time. To proceed with solving for the appropriate \\(l\\), we need to make one assumption. This is that any two image series with the same mean intensity profile are appropriately detrended with the same detrending parameter \\(l\\). This assumption seems reasonable to me, however there is no need to debate its validity because later, detrending with the solved-for parameter \\(l\\) will be evaluated with simulated data and compared to the standard \\(l = 10\\). If this assumption is bad, then the performance of the detrending that relies on it should also be bad. With this assumption in hand, solving for \\(l\\) proceeds as follows: Simulate an image series with immobile particles only which has the same mean intensity profile as the acquired real data. Given that the simulated series is of immobile particles only, once properly detrended, it should have \\(B = 1\\). The \\(l\\) for which the detrended series has mean brightness closes to 1 is the most appropriate for the simulated data. By the assumption above, this \\(l\\) is the most appropriate for the real data. Mathematically, this can be expressed as \\[\\begin{equation} l = \\text{argmin}_{\\tilde{l}} (1 - |\\text{ mean brightness of simulated series detrended with parameter} \\tilde{l}|) \\tag{3.1} \\end{equation}\\] In fact, what I have done here is to give a general method for solving for any detrending parameter \\(\\alpha\\): \\[\\begin{equation} \\alpha = \\text{argmin}_{\\tilde{\\alpha}} (1 - |\\text{ mean brightness of simulated series detrended with parameter} \\tilde{\\alpha}|) \\tag{3.2} \\end{equation}\\] This will be useful later when other detrending regimes with their own parameters are introduced. 3.3.4 Exponential smoothing Exponential smoothing is a slight alteration to boxcar smoothing. The idea is that when computing a local average, points nearer to the point of interest should have greater weights.13 The weights fall off with distance \\(|t|\\) from the point of interest according to \\(\\exp(-\\frac{|t|}{\\tau})\\) where the parameter \\(\\tau\\) is a positive real number. This function is visualized in figure (fig:exp-smooth-func). For small values of \\(\\tau\\), only values very close to the point of interest have importance when calculating the local average. For larger values of \\(\\tau\\), further values also have importance (but closer values always have higher rates). In this sense, increasing the value of \\(\\tau\\) has a similar effect to increasing the value of \\(l\\) for the boxcar in that farther away points are taken into account. Figure 3.10: The function \\(\\exp(-\\frac{|t|}{\\tau})\\) visualised with \\(\\tau = 3\\) and \\(\\tau = 9\\). For \\(\\tau = 3\\), points at distance \\(|t| = 10\\) have approximately zero weight, whereas for \\(\\tau = 9\\), these points have significant weight. In figure 3.11, exponential smoothing with different parameters \\(\\tau\\) is applied to the trace in figure 3.4. The results are similar to those in figure 3.9. Figure 3.11: The original intensity trace is shown in the top-left. The other panels show the result of exponential smoothing for \\(\\tau = 1, 3, 75, 300\\) and \\(1000\\). Heuristically, exponential smoothing detrending seems favourable to boxcar detrending because the idea that points further away from the point of interest are less important (but still somewhat important) when computing the local average is reasonable. Indeed, this was the method proposed in the original number and brightness paper (Digman et al. 2008). For this reason, exponential smoothing was the method of choice for my paper where the method of choosing the correct detrending parameter was published (Nolan, Alvarez, et al. 2017). 3.3.5 Correcting for non-stationary variance Definition 3.4 The variance of a random variable \\(X\\) is the expected value of the squared deviation of \\(X\\) from its mean \\(\\mu\\): \\[\\begin{equation} \\text{Var}(X) = E[(X - \\mu)^2] \\tag{3.3} \\end{equation}\\] All of this chapter so far has focused on correcting for non-stationary mean. As shown in figure 3.3, as the mean decreases, so too does the variance. For an instance \\(x\\) of the random variable \\(X\\), \\(x - \\mu\\) is the deviation of \\(x\\) from \\(\\mu\\). If we write \\(x\\) as \\(x = \\mu + \\tilde{x}\\), then we get the deviation \\(x - \\mu = (\\mu + \\tilde{x}) - \\mu = \\tilde{x}\\), so \\(\\tilde{x}\\) is the deviation. For a given point in figure 3.3, its deviation is its distance from the black line. For positive real number \\(k\\), making the transformation \\(\\tilde{x} \\rightarrow \\sqrt{k}\\tilde{x}\\) i.e. \\(x \\rightarrow \\mu + \\sqrt{k}\\tilde{x}\\) causes the variance (i.e. the squared deviation) to be transformed as \\(\\text{Var}(X) \\rightarrow k \\times \\text{Var}(X)\\). Hence, we have a way to modify the variance of a time series as a whole by modifying the deviation of each time point from the mean. For months, I toyed with this idea as a solution of correcting for non-stationary variance. However, in reality the contribution to the variance in intensity at a given pixel is down to both poisson photon statistics and fluorophore movement. This combination of factors makes it very difficult to ascertain the amount by which the variance should be altered. I eventually abandoned my efforts to alter the variance like this in favour of the Robin Hood detrending algorithm (section 3.3.6) which includes correction for non-stationary variance as an intrinsic part of its detrending routine. 3.3.6 Robin Hood detrending References "],
["applications.html", "Chapter 4 Applications 4.1 Example one 4.2 Example two", " Chapter 4 Applications Some significant applications are demonstrated in this chapter. 4.1 Example one 4.2 Example two "],
["final-words.html", "Chapter 5 Final Words", " Chapter 5 Final Words We have finished a nice book. "],
["references.html", "References", " References "]
]
